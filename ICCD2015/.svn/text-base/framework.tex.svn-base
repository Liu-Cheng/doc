\section{QuickDough Framework}\label{sec:framework}
QuickDough is an FPGA loop accelerator generation framework. It generates FPGA accelerators for
compute intensive loop kernels in the order of seconds through the use of an SCGRA overlay. It also
generates the drivers of the accelerators automatically, integrating both software and hardware
generation in a unified framework.

\subsection{QuickDough Overview}
\figref{fig:framework} illustrates the proposed FPGA loop accelerator design framework named QuickDough.
It roughly consists of a conventional software compilation path, a fast FPGA loop accelerator
generation path and a slow accelerator library update path. The first path of QuickDough is to compile
the overall software application to the host processor after replacing the compute kernels with
calls to the accelerator drivers which mainly control and transfer data to and from the accelerator. 

\begin{figure}[bt]
    \center{\includegraphics[width=0.8\linewidth]{framework}}
    \caption{QuickDough: FPGA loop accelerator design framework using 
        SCGRA overlay. The compute intensive loop kernel of an 
        application is compiled to the SCGRA overlay based FPGA
    accelerator while the rest is compiled to the host processor.}
    \label{fig:framework}
\end{figure}

The second path of QuickDough is a rapid and common route for loop accelerator generation. To begin,
the compute intensive loop kernel is statically transformed to the corresponding data flow graph
(DFG) with specified loop unrolling factor. Then the DFG will be compiled to an accelerator
with user specified SCGRA overlay size. After the scheduling, the number of cycles needed to
complete the DFG computation will be determined and the configuration requirements of the
accelerators that fit the DFG can be decided. Finally, the most appropriate pre-built accelerator
will be selected from the library and the corresponding scheduling result is embedded directly into
the pre-built accelerator to create the final FPGA configuration bitstream. This bitstream, in
combination with the software created in the first path, forms the final application that will be
executed on a target CPU-FPGA system.

The third path of QuickDough is to update the accelerator library upon users' 
request. Users may simply provide the hardware resource budget. 
Then target operations to be supported will be decided automatically by analyzing
the DFGs produced by the DFG generator. With the resource budget and the supported 
operation set, a set of accelerator HDL models will be generated by utilizing the 
overlay template. Finally, the accelerator HDL models are implemented on target 
FPGA platforms and further updated to the accelerator library.

\subsection{SCGRA overlay based FPGA accelerator}
The QuickDough overlay consists of an array of simple processing 
elements (PEs) connected by a direct network executing 
synchronously as shown in \figref{fig:scgra-accelerator}.
Each PE computes and forwards data in lock steps, allowing deterministic 
multi-hop data communication that overlaps with computations.
The action of each PE in each cycle is controlled by an instruction 
ROM that is populated with instructions generated by the design framework.
Finally, a data memory is featured on each PE to serve as a temporary 
storage for run-time data that may be reused in the same PE or be 
forwarded in subsequent steps.

\begin{figure}[tb]
    \center{\includegraphics[width=0.65\linewidth]{scgra-accelerator}}
    \caption{SCGRA Overlay Based FPGA Accelerator}
    \label{fig:scgra-accelerator}
\end{figure}

\begin{figure}[tb]
\center{\includegraphics[width=0.65\linewidth]{pe}}
\caption{Fully pipelined PE structure. Each PE can be connected to at most 4 neighbours.}
\label{fig:pe}
\end{figure}

\begin{figure}[tb]
\center{\includegraphics[width=0.65\linewidth]{alu-v2}}
\caption{The QuickDough ALU. It supports up to 16 fully pipelined 3-input operations.}
\label{fig:ALU}
\end{figure} 

Communication between the accelerator and the host processor is 
carried through a pair of input/output buffers.
Accesses to these I/O buffers from the SCGRA array take place in 
lock step with the rest of the system.
The exact buffer location to be accessed is control by the 
AddrIBuf and AddrOBuf blocks. Both of them are ROM populated with 
address information generated from the QuickDough compiler.

\subsubsection{PE template}
\figref{fig:pe} shows the current implementation of a QuickDough 
PE template that features an optional load/store path. 
At the heart of the PE is an ALU, which is supported by a multi-port 
data memory and an instruction memory.
Three of the data memory's read ports are connected to the ALU as inputs, 
while the remaining ports are sent to the output multiplexors for 
connection to neighboring PEs and the optional store path to 
OBuf external to the PE. At the same time, this data memory takes 
input from the ALU output, data arriving from neighboring PEs, as well 
as from the optional IBuf loading path.
The action of the PE is controlled by the AddrCtrl unit that reads from the instruction memory.
Finally, a global signal from the AccCtrl block controls the start/stop of all PEs in the array.

\subsubsection{ALU template}
At the heart of the proposed PE is the ALU and it can easily be 
customized with different operations specifically for any given user 
applications. \figref{fig:ALU} shows the ALU template used 
in the QuickDough overlay. These operators in the ALU may execute 
concurrently in a pipelined fashion and must complete in a 
deterministic number of cycle. Given the deterministic nature of 
the operators, the QuickDough scheduler will 
ensure that there is never conflict at the output multiplexor.

\subsection{Loop execution on the accelerator}
The loop kernels are mostly partially unrolled, transformed to DFG and scheduled to the SCGRA
overlay of the accelerator. A straightforward way to complete the whole loop computation on top of the SCGRA
overlay is to repeat the same DFG computation until the end of the loop. Nevertheless, this may
require data transfer between host processor and I/O buffer for each DFG computation. As a result,
the communication cost increases dramatically especially when the amount of each data transfer is
small. Worse still, input data of the consecutive DFGs may be reused and the straightforward
data transfer strategy may greatly increase the total amount of data transfer through out the loop
computation. 

To alleviate this problem, we have proposed to batch data transfers for multiple executions of the
same DFG into groups as shown in \figref{fig:blocking-and-dfg-gen}. Specifically, after the loop is
unrolled $U$ times, $G$ of them are grouped together for each data transfer. This group strategy
helps to amortize the initial communication cost between host processor and the accelerator. In
addition, it allows input data to be reused for different DFG computation in the same group and the
group size is mainly limited by the I/O buffer depth. Meanwhile, the accelerator communicates with
host processor for each group execution, and thus the accelerator drive that handles the communication
depends on the I/O buffer depth as well. Clearly, accelerator with larger I/O buffer is
preferable when the rest part of the accelerator configuration fulfills the requirements. 


\begin{figure}
\center{\includegraphics[width=0.75\linewidth]{dfg-gen}}
\caption{Loop execution on an SCGRA overlay based FPGA accelerator}
\label{fig:blocking-and-dfg-gen}
\end{figure}


\subsection{FPGA loop accelerator generation}
In order to produce an FPGA accelerator rapidly for a specified compute intensive loop kernel, DFGs
are extracted from the kernel that is often expressed as inner loop body. The users may further
unroll the loops multiple times to increase the amount of operation parallelism in the generated
DFG. In this work, we have developed a C++ library to help automate the DFG generation with
specified loop unrolling factor.

\subsubsection{DFG Scheduling}
Given the specified SCGRA size, the operations from the user DFG are scheduled to execute on the
SCGRA overlay based accelerator. Since the PEs in the overlay execute in lock steps with
deterministic latencies and the target DFGs typically include thousands of nodes, a classical list
scheduling algorithm \cite{schutten1996list} was adopted. A scheduling metric as presented in
\cite{colinheart}, considering both load balancing and communication cost was adopted in our
current implementation. After the scheduling, the number of cycles needed to complete the DFG
computation can be determined and the minimum depth of the instruction memory can be decided
accordingly. 

\subsubsection{Accelerator selection}
When minimum depth of the instruction memory is determined after the scheduling, any accelerator
configuration that fulfills the instruction memory depth and SCGRA size requirements can be a
candidate. To ensure a rapid accelerator generation, the accelerator configuration with the largest
I/O buffers will be selected from the candidates because larger I/O buffers are usually beneficial to the
overall loop performance as discussed in previous section. When the I/O buffers are determined,
grouping and the amount of communication between host processor and the accelerator can be
determined. Therefore, the corresponding accelerator drivers can be generated as well.

However, it is also possible that there is no candidate that meets the SCGRA size and instruction
memory depth requirement. In this case, an accelerator with smaller SCGRA overlay will be selected
automatically. The DFG scheduling and accelerator selection will be repeated until a suitable
accelerator is found. If it turns out that none of the accelerators in the library fits the loop
kernel, the user may decrease the loop unrolling factor or enlarge the resource budget and update
the accelerator library accordingly. The accelerator library update will be discussed in the
following section. The iterative selection process may increase the accelerator
generation time. Fortunately, most of the time an accelerator can be found successfully in the
first round selection when the loop is not intensively unrolled.

\subsubsection{Accelerator bitstream generation}
The final step of the accelerator generation is to generate 
the instructions for each PE and the address sequences for the 
I/O buffers according to the scheduler's result, which will subsequently 
be incorporated into the configuration bitstream of the overlay produced 
from previous steps. Then we take advantage of the reconfigurability 
of SRAM based FPGAs and store the cycle-by-cycle configuration words 
using on-chip ROMs. The content of the ROMs are embedded in the 
bitstream and the \code{data2mem} tool from Xilinx \cite{data2mem} is 
used to update the ROM content of the pre-built bitstream directly. 
To complete the bitstream integration, \code{BMM} file that describes 
the organization and placements of the ROMs in the overlay is extracted 
from \code{XDL} file corresponding to the overlay implementation \cite{beckhoff2011xilinx}.
This bitstream integration process costs only a few seconds of the compilation time.

\subsection{Accelerator library update} 
Accelerator library consists of a number of pre-built SCGRA overlay based accelerators with
different configurations. It is the basis for the proposed rapid FPGA loop accelerator generation
framework. In this section, we will illustrate how the accelerator library is updated
given the hardware resource budget and target loop kernels.

Accelerator library update is essentially to pre-implement a group of SCGRA overlay based FPGA
accelerators upon users' request, which may either target a specified application or a domain of
applications. Since QuickDough design framework is developed to enhance the
designers' design productivity and make it accessible to high-level designers, the library
update which involves low-level overlay design and implementation must be automated so that it
will not become a new barrier to the application developers.  
.
\figref{fig:auto-lib-gen} presents the proposed automatic accelerator library update flow. It
roughly consists of four steps i.e. DFG generation, common operation analysis, minimum accelerator
configuration set analysis, and accelerator HDL model generation and implementation. Since DFG generation
has been discussed in previous section, we will mainly detail the rest three steps in this section.
\begin{figure}
    \center{\includegraphics[width=0.45\linewidth]{lib-gen}}
\caption{Automatic SCGRA overlay based FPGA accelerator library update}
\label{fig:auto-lib-gen}
\end{figure}

\subsubsection{Common operation analysis}
Assume that the operations used to construct the DFG is up to the DFG generation process, the common
operation analysis step mainly decides the minimum operation set that is needed to support the
target applications. It is possible to co-optimize the DFG generation and common operation set
analysis, but it is beyond the discussion of this work. Currently, we just perform a union of the
operation types included in the DFGs. It is trivial, but the minimum operation set can be decided
automatically and rapidly.

\subsubsection{Minimum accelerator configuration set analysis}
Although the library can be implemented off-line, it does takes a long time to complete.
Therefore, we try to find out the minimum set of accelerator configurations that 
can cover as many scenario as possible. The proposed SCGRA overlay based FPGA accelerator utilizes
block RAM to implement the instruction memory, data memory, on-chip buffer as well as the address
buffer, and block RAM is the hardware resource bottleneck. As a result, the minimum set of
accelerator configurations eventually depends on how the block RAMs are allocated to these
components. In addition, an accelerator with smaller configuration on all the design parameters can
be replaced by the one with larger configuration, and thus we can just implement the accelerators
that reach the block RAM limit. Moreover, the block RAM resource available on FPGA are discrete and
empirical settings such as limiting data memory to a single primitive block RAM, constraining the
difference between SCGRA row size and column size, which further shrinks the number of accelerators
pre-built in the library.  

\subsubsection{Accelerator HDL model generation and implementation}
With the proposed SCGRA overlay template and the accelerator configurations to be pre-built in the
library, corresponding HDL models of the SCGRA overlay based FPGA accelerators are generated with a
python script. Then the library can be implemented using the conventional hardware implementation
tools. The lengthy implementations can be done in parallel. Moreover, the regular tiling structure
even allows the implementations to be accelerated using macro based implementation techniques as
presented in \cite{ROB2014}, which can be up to 20X faster than a standard HDL implementation with
negligible timing and overhead penalty.

