\section{Introduction} \label{sec:introduction}
Deep neural networks (DNNs) have been demonstrated to be successful in 
massive territories like image processing, video processing, and 
natural language processing over the years \cite{Gatys_2016_CVPR, Collobert:2008:UAN:1390156.1390177, 
Chen_2015_ICCV}. The success further provokes the 
flourish of customized neural network accelerators 
with massive parallel processing engines which typically offer
much higher performance and energy efficiency compared to 
general purposed processors (GPPs) 
\cite{chen2014diannao, chen2014dadiannao,  chen2016eyeriss, Zhang:2015:OFA:2684746.2689060, 7551397}.
While the performance of neural network accelerators has been intensively 
optimized from various angles like pruning and quantization, 
the reliability of the accelerators especially 
the ones that are deployed on FPGAs remains not well-explored.

The capacity of FPGAs increases dramatically with the 
advancements of semiconductor technology, but the integrated 
circuits with smaller feature sizes are more susceptible to 
manufacturing defects and abnormal physical processes like 
thermal stress, electromigration, hot carrier injection, 
and gate oxide wear-out. Thereby, the 
persistent (hard) faults become one of the major sources 
of system unreliability. Despite the fault-tolerance of 
neural network models, persistent faults in neural network 
accelerators can cause computing errors, considerable 
wrong predictions \cite{238315} and system exceptions. 
They may even lead to disastrous 
consequences to some of the mission-critical applications such as 
self-driving, nuclear power plants, and medical diagnosis. 

Understanding the consequences of hardware faults on neural network accelerators 
is an indispensable step to improve the resilience of the neural 
network models executed on the accelerators. There are already some 
prior works that present analysis of hardware faults on neural network 
accelerators for this purpose \cite{Reagen2018, Kausar2016Artificial, Li2018TensorFI, 
Li2017, Salami2018}. However, they usually experimented with 
simulators and focused on the influence of hardware faults on the 
prediction accuracy of the neural network models. 
For instance, the authors in \cite{Reagen2018} 
mainly investigated the data faults of weights, activities, 
and hidden states, which are stored in on-chip buffers.
Then they explored the neural network model resilience with 
model-wise analysis and layer-wise analysis. The work 
in \cite{Li2017} targeted at the analysis of software errors 
in DNN accelerators and explored the error propagation 
behaviors by comparing the different structures of the neural networks, 
data types and so on. 

The simulation based approaches are fast and flexible for analyzing 
neural network computing and model prediction accuracy, but they typically 
ignore critical controlling details and interfaces of 
the DNN accelerators to the memory and the attached GPPs to 
ensure the faster simulation speed. Otherwise, the large amount of 
neural network computing may take extremely long time to simulate on 
a practical data set. Nevertheless, hardware faults may have considerable 
influence to the overall acceleration system other than the prediction 
accuracy loss. For instance, faults in DMA 
module may result in illegal memory accesses and corrupt 
the system especailly for shared memory structures.
This must be addressed to guarantee reliable DNN 
acceleration. In addition, many DNN accelerators are implemented on FPGAs for 
more intensive customization and flexible reconfiguration. 
While the functionality of the DNN accelerators is mapped to the 
FPGA infrastructures instead of the primitive logic gates, 
the simulation based fault analysis approaches that usually inject errors to the 
operations used in DNN processing do not apply to the 
FPGA based DNN acceleration system. Because hardware faults 
in FPGAs affect the configuration of the devices instead of the 
accelerator components directly. The actual parts of the 
accelerators that are influenced by faults also depend on 
the FPGA placing and routing. 

To gain further insight of hardware faults in a DNN acceleration system, 
we conduct the fault analysis on a running ARM-FPGA system 
where FPGA has a representative DNN accelerator with 2D systolic array 
implemented along with hardware fault injection modules and shares 
the DRAM with the ARM processors. The fault injection modules can inject
random bit errors to both the configuration memory and on-chip memory 
blocks in either an offline manner or an online manner with the equipped 
AXI slave ports. The fault distribution models are implemented 
with software on the ARM processors, so they are convenient to change for fault 
analysis using different models. On top of the FPGA-based fault 
analysis platform, we take four typical neural network models 
including Yolo, Resnet, LSTM and DCGAN that represent a broad range of 
neural network applications as a benchmark for fault analysis.
Unlike prior works that focused on prediction accuracy loss analysis, 
we try to analyze the behaviours of the DNN accelerators under hardware faults
from a system point of view and analyze the underlying reasons that lead 
to these different consequences of the system. Particularly, we study the 
system exceptions that dramatically destroy the system 
functionality in detail and present a simple yet efficient approach 
to alleviate them.

The contributions of this work are summarized as follows: 
\begin{itemize}
  \item We develop a configurable fault injection and analysis system 
    targeting fault analysis of neural network accelerators on ARM-FPGAs. 
    It offers high-level interfaces for both fault injection and 
    output data comparison of neural network models to facilitate 
    the systematic fault analysis.

  \item On top of the fault analysis system, we mainly classify 
    and analyze the resulting behaviors of the DNN accelerators 
    from system point of view. We particularly emphasize the 
    analysis of the different system exceptions and investigate the 
    underlying reasons.

  \item With comprehensive experiments on a set of representative 
    neural network models, we observe that system exceptions can 
    destroy the functionality of the DNN acceleration system 
    other than model prediction accuracy loss and they are mainly 
    caused by the faults in configuration memory 
    of the FPGA and instruction memory of the DNN accelerator. 
    With this observation, we propose a baseline error 
    correction code (ECC) protection approach with negligible 
    hardware overhead and demonstrate the significant 
    resilience improvement of the system.
\end{itemize}

The rest of the paper is organized as follows. In Section \ref{sec:background}, 
we introduce a typical DNN accelerator architecture with 2D computing array. 
In Section \ref{sec:fault-analysis}, 
we detail the proposed fault analysis framework on ARM-FPGAs and show the 
various system conseqauences especially the system exceptions caused by 
persistent hardware faults. In Section \ref{sec:experiment}, we present the 
experiment results and analyze the underlying reasons. In Section \ref{sec:relatedwork}, 
we give a brief review of prior fault tolerant analysis and design of neural network accelerators.
Finally, we conclude this work in Section \ref{sec:conclusion}.

