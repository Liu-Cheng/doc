\section{Experiments}
In this section, we will analyze the benefits of 
overclocking on a set of representative convolution neural networks.
Meanwhile, we will also estimate the trade-off of the required strategies 
used to mitigate the overclocking errors.

\subsection{Experiment setup}
We use PipeCNN as the baseline CNN accelerator and have it implemented on KCU1500 FPGA board.
Four representative neural networks including LeNet, AlexNet, VGG-16 and VGG-19 are used to 
benchmark the performance and energy efficiency of the accelerator. The implementation clock
frequency of LeNet and AlexNet is 210 MHz while the clock frequency for VGG-16 and VGG-19 is 190 MHz.
Then we gradually overclock the implementation with 10 MHz step until the accelerator gets 
stuck or crashes frequently. For the 210 MHz implementation, the highest overclocking setup is 
260MHz. For the 190 MHz implementation, the highest overclocking setup is 240 MHz.
Finally, we evaluate the performance and energy efficiency of the accelerators using 
all the available overclocking configurations.

\subsection{Accuracy, performance and energy efficiency}
The prediction accuracy of the benchmark neural networks on the CNN accelerators with overclocking is 
presented in Fig \ref{fig:overclock-accuracy}. It can be found that the prediction accuracy 
regardless of top1 or top5 typically remains the same under moderate overclocking. However, when the clock 
continues to rise, it may reach to a tipping point where the prediction accuracy drops clearly. 
Fortunately, the prediction accuracy can be improved with just on-accelerator retraining.
When the clock further increases, the prediction accuracy drops dramatically. In spite of the retraining, 
the prediction accuracy is still not acceptable in practice. 

\begin{figure}
        \center
	\subfloat[LeNet]{
		\label{fig:lenet_accuracy}
		\includegraphics[width=0.75\linewidth]{accuracy_lenet}
	}
	\qquad
	\subfloat[AlexNet]{
                \label{fig:alexnet_accuracy}
                \includegraphics[width=0.75\linewidth]{accuracy_alexnet}
        }
	\qquad
	\subfloat[VGG-16]{
                \label{fig:vgg16_accuracy}
                \includegraphics[width=0.75\linewidth]{accuracy_vgg16}
        }
        \qquad
	\subfloat[VGG-19]{
                \label{fig:vgg19_accuracy}
                \includegraphics[width=0.75\linewidth]{accuracy_vgg19}
        }
	\caption{The prediction accuracy of the benchmark neural networks on accelerators with different overclocking}
        \label{fig:overclock-accuracy}
\end{figure}

We further evaluate the normalized performance over the original CNN accelerators.
As given in \ref{fig:relative_time_overclock}, it can be found that the performance with the extreme accelerator 
overclocking is 1.25 on average. 
Finally, we use EDP as the energy efficiency metric and compare the 
different overclocking configurations. The comparison is exhibited in \ref{fig:relative_energy_overclock}.
According to the comparison, we can conclude that overclocking on FPGA based CNN 
accelerators improves both performance and energy efficiency with negligible or small 
predication accuracy loss. In addition, the experiments exhibit that prediction 
accuracy has a cliff-like drop with the increasing overclocking frequency. 

\begin{figure}
        \center{\includegraphics[width=0.75\linewidth]{relative_time_overclock}}
    \caption{Normalized performance of neural networks executed on CNN accelerators with different overclock.}
\label{fig:relative_time_overclock}
\vspace{-1em}
\end{figure}

\begin{figure}
        \center{\includegraphics[width=0.75\linewidth]{relative_energy_overclock}}
    \caption{Normalized EDP of neural networks executed on CNN accelerators with different overclock.}
\label{fig:relative_energy_overclock}
\vspace{-1em}
\end{figure}

In order to quantize the exact computing errors caused by overclocking, we 
particularly analyze the last layer output of the neural networks which 
is typically a vector. We compare it with the output without overclocking.
And we use the percentage of changed output data and the Euclidean distance to 
characterize the difference of the output vector. 
The comparison is shown in \ref{tab:fr_ed}.

\begin{table}
        \centering
        \vspace{-0.3em}
        \caption{Fault rate and Euclidean distance on the last output layer of the neural network}
        \label{tab:fr_ed}
        \vspace{-0.3em}
        \begin{tabular}{c|cccccc}
                \toprule
                Frequency(MHz) & 210 & 220 & 230 & 240 & 250 & 260 \\
                \midrule
                Euclidean distance & 0 & 0 & 0 & 0 & 35.2 & 104.8 \\
		\midrule
                Fault rate(\%) & 0 & 0 & 0 & 0 & 56.6 & 72.1 \\
                \bottomrule
        \end{tabular}
        \vspace{-1em}
\end{table}

As discussed in this paper, the timing error can be affected by many factors and it may change 
at runtime. There is no guarantee that the behavior of the CNN accelerator can keep stable even 
overclocking is just slightly higher than the original clock. We use the accelerator overclocked at the 
tipping point to perform the neural network computing. Then we keep measuring its 
prediction accuracy. As shown in \ref{fig:stability}, we find that the accuracy is rather stable.
Although we still can not ensure the stability of the overclocked CNN accelerator, we can 
be sure that the probability of the sever errors such as accelerator hangup or considerable 
accuracy loss is rather low. As we did not observe the cases after processing 1000000 pictures, 
we assume the probability of an severe error when processing an input picture is lower than 1/100000.

\begin{figure}
	\center{\includegraphics[width=0.75\linewidth]{stability}}
    \caption{Overclocking stability analysis}
\label{fig:stability}
\vspace{-1em}
\end{figure}

\subsection{Optimization tradeoffs}
With the severe error probability, we further evaluate overhead of 
the proposed error detection and error recovery strategy. The overhead 
is defined as Equation\ref{eq:cost}.

\begin{equation}
        \label{eq:cost}
L=B/P+R/B+R/P+1
\end{equation}

Where B is Block size, P is fault rate, R is reference data size. As shown in \ref{fig:cost_block}
Under a certainly fault rate, the overhead can achieve min value with a suitable block size. 
So we evaluate the overhead with suitable block size under different fault rate.  
As shown in \ref{fig:cost_probability}, the overhead is declining exponentially with probability.
Because the fault rate is increase with overclocking, so more overclocking will make less overhead.

\begin{figure}
        \center{\includegraphics[width=0.75\linewidth]{cost_batch}}
    \caption{The cost of overclock with different block size.}
\label{fig:cost_block}
\vspace{-1em}
\end{figure}

\begin{figure}
	\center{\includegraphics[width=0.75\linewidth]{cost_probability}}
    \caption{The cost of overclock with different fault rate.}
\label{fig:cost_probability}
\vspace{-1em}
\end{figure}

