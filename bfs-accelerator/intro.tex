\section{Introduction} \label{sec:intro}
Breadth-first search (BFS) is the basic building component of many graph algorithms 
and is thus of vital importance to high-performance graph processing. Nevertheless, 
it is notoriously difficult for accelerating on FPGAs because of the 
irregular memory access and the low computation-to-memory ratio. 
At the same time, BFS on large graphs also involves tremendous 
parallelisms which indicate great potential for acceleration. 
With both the challenge and the parallelization potential, 
BFS has attracted a number of researchers exploring its acceleration on FPGAs 
\cite{attia2014cygraph, betkaoui2012reconfigurable, Dai2017foregraph, Ma2017fpga,
umuroglu2015hybrid, oguntebi2016graphops, engelhardt2016gravf, zhou2016high}. 

Previous work have shown that BFS accelerators on FPGAs can provide competitive  
performance and superior energy efficiency when given comparable memory bandwidth. 
However, these work typically optimize BFS or relatively general graph processing 
with dedicated circuit design using hardware description language (HDL). The HDL 
based customized designs are beneficial to the resulting performance 
and save resource consumption, but it usually takes long time for development, 
upgrade, maintenance and porting to a different FPGA device, which are all 
important concerns from the perspective of the accelerator developers. 

%Another engineering yet non-trivial problem is the high barrier to use 
%the FPGA powered graph processing accelerators in high-level 
%applications such as big data analytics, which is mostly caused by the lack of 
%well-defined high level interface and user-friendly SDK supporting 
%various hardware systems. Improving the ease of using the 
%HDL based accelerators requires a lot of design efforts 
%such as driver and runtime environment to support newer 
%devices and diverse computing systems. This is also one of 
%the key obstacles hindering the widespread adoption of 
%the FPGA accelerators despite the great performance-energy 
%efficiency advantages.

Because of the limitation of the conventional HDL design method, 
high level synthesis (HLS) tools that advance rapidly in recent years
become attractive. HLS tools are increasingly adopted in both 
industry and academia for rapid FPGA 
prototyping and application acceleration \cite{koch2016fpgas, xilinx-sdaccel}. 
Nevertheless, the current HLS based design tools are mostly used for 
applications with relatively regular memory access patterns and data paths. 
It remains challenging for the HLS tools to accelerate BFS with irregular 
memory access patterns and complex data paths. 

First of all, there are considerable irregular memory access including 
random and short burst memory access in BFS and the irregular memory access 
bandwidth utilization is orders of magnitude lower than that of the sequential 
memory access. In addition, the number of a 
frontier vertex's neighbor is only determined at run-time, so reading the unaligned 
neighboring vertices with OpenCL typically has narrow data width. The read is not efficient 
as pointed out in \cite{wang2017multikernel}. Worse still, the neighboring vertices 
may have their visiting status stored randomly either in main memory or on-chip buffer.
Although they can be processed in parallel, read/write conflicts happen frequently 
and affects the BFS data path parallelization dramatically. 
The authors in \cite{ham2016graphicionado} 
proposed to resolve the parallelization and conflict problem with a 
crossbar and a set of input/output buffers. However, the method implemented 
with OpenCL suffers long pipelining delay and large initialization interval. 

To solve this problem, we reorder each vertex's neighboring vertices i.e. 
edges in compressed sparse row (CSR) graph and split them into 
batches such that the neighboring vertices 
in each batch have their status stored in independent memory blocks. 
With the batching, the random memory access gets coalesced. Meanwhile, 
the data path can be simplified and implemented in parallel efficiently. 
We also observe that updating the BFS level information of the 
active vertices is essentially random memory access and time-consuming 
in BFS while the updating can be done anytime when the active vertices of BFS 
are determined. Thus, we can separate the random memory operations from
the conventional BFS and have the update overlapped with the next BFS processing. 
With the hyper pipelining, the random memory operations can be completely 
hidden, which is beneficial to the BFS processing.

According to the experiments on a set of big graphs, the proposed OpenCL based BFS 
accelerator achieves up to 12X performance speedup when compared to the design 
in Spector benchmark and it is on par with many handcrafted design on similar FPGA boards. 
The major contributions of this work are summarized as follows.
\begin{itemize}
    \item As far as we know, this is the first highly optimized and open-sourced 
		OpenCL based BFS accelerator on FPGAs targeting portability and ease of 
		use on top of performance. 
    \item We proposed a set of methods to optimize the irregular
		memory accesses and data path parallelization of BFS using OpenCL. 
		This may shed light on similar irregular application acceleration on 
		FPGAs using OpenCL.
    \item The resulting accelerator shows significant performance speedup 
        over a baseline OpenCL design and gets close to state-of-art handcrafted 
		design on a set of representative graphs.
\end{itemize}

The rest part of the paper is organized as follows. In Section \ref{sec:background}, 
we brief the background of software programmable FPGAs and baseline BFS accelerator. 
In Section \ref{sec:motivation},  
we demonstrate the challenge of BFS acceleration using HLS tools with our own experience.
In Section \ref{sec:bfs-opt}, we detail the major OpenCL based BFS optimization methods.
In Section \ref{sec:experiment}, we present comprehensive experiments of the 
BFS accelerator. In Section \ref{sec:relatedwork}, we present a brief survey of 
the graph processing accelerators particularly on FPGAs. Finally, we conclude 
this work in Section \ref{sec:conclusion}.

