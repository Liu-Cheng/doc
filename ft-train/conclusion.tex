\section{Conclusion} \label{sec:Conclusion}
In this work, we aim to improve the resilience of the neural networks to enable 
more advantageous design trade-offs through retraining. 
we propose to replace the forward computing on GPPs with accelerator computing during training and have both the computing 
errors and the application data learned in the neural network models. 
In addition, we opt to protect critical neural layers to reduce the negative 
influence of computing errors.  
With the proposed resilient neural network training, 
the prediction accuracy of the retrained neural network models improves significantly 
when computing errors appear. 


%\appendix
%\section{Acknowledgement}

%\begin{acks}
%  The authors would like to thank Sam Ho for providing the suggestions on
%  HLS design debugging and optimization as well as the SDAccel usage. 

%\end{acks}
