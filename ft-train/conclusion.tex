\section{Conclusion} \label{sec:Conclusion}
The inherent fault-tolerance of neural networks allows design trade-offs between 
minor prediction accuracy loss and great performance or energy efficiency improvements. 
In this work, we aim to further improve the resilience of the neural networks to enable 
more advantageous design trade-offs through retraining. Unlike conventional offline training on GPPs, 
we propose to replace the forward computing on GPPs with accelerator computing during training and have both the computing 
errors incurred by the relaxed design constraints and the application data learned in the neural network models. 
In addition, we opt to protect critical neural layers to reduce the negative 
influence of computing errors. This strategy elevates the 'shortest wooden bar' of 
the neural networks and improves the overall resilience. With the proposed resilient neural network training, 
the prediction accuracy of the retrained neural network models improves significantly 
when computing errors appear. We further demonstrate the superiority of the proposed training 
with overclocking on FPGA based CNN accelerators using a set of realistic neural network models.

%\appendix
%\section{Acknowledgement}

%\begin{acks}
%  The authors would like to thank Sam Ho for providing the suggestions on
%  HLS design debugging and optimization as well as the SDAccel usage. 

%\end{acks}
