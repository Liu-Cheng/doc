\section{Experiments} \label{sec:casestudy}
In this section, we evaluate the proposed resilient neural network training 
framework for accelerators with computing errors. The errors can be caused by 
various relaxed design constraints and we use random computing errors in 
the experiments for general analysis. 

\subsection{Experiment setup}
We experiment on 8bit fixed-point PipeCNN \cite{pipecnn_2} accelerators on Xilinx KCU1500.
The FPGA board is attached to Intel(R) Core(TM) i7-6700 CPU @3.40GHz with 32GB memory.
To simulate general hardware errors caused by relaxed design constraints, we 
inject random bit errors to input/intermediate/output features and weights as well as 
hidden layer status of neural networks. The error injection is measured with 
bit error rate (BER) which is also utilized in \cite{B2018ARES}. Compared to \cite{B2018ARES},
we also have random errors injected to the internal computing results i.e. the hiden status. 
To evaluate the training, we take three representative convolution 
neural networks including AlexNet, VGG-16 and VGG-19 as the benchmark. 
The analysis can be applied to more neural networks.

\subsection{Neural network resilience analysis}
To explore the resilience of the proposed neural network training, we
compare the prediction accuracy of neural networks in three scenarios.
In the first case, we have offline trained neural network models deployed on 
CNN accelerators with computing errors directly as denoted as 'original'.
In the second case, we have the neural network models retrained on the 
accelerator with computing errors. It is represented as training with 
accelerator (TWA). In the third case, we have the critical layers 
protected on top of the second case. Basically, we schedule the critical layers to 
GPPs to ensure precise computing during both retraining and inference.
It is denoted as critical layer protected(TWA+CLP).

The comparison of the three cases is presented in Figure \ref{fig:softerror-accuracy}.
When the BER goes up, the prediction accuracy of the original neural network drops 
considerably despite the resilience of the neural networks. 
With the proposed training i.e. TWA+CLP, the top1 and top5 precision accuracy 
of the retrained models improves by 20.7\% and 5.9\% on average respectively 
compared to the offline trained model under the highest error injection rate. 
The great prediction accuracy improvement indicates that the resilience 
of the retrained neural network models is improved targeting at the 
specific computing error pattern. Therefore, more aggressive design trade-offs 
between prediction accuracy and performance or energy efficiency can be performed. 
\begin{figure}
        \center
		\subfloat[AlexNet]{
                \label{fig:alexnet}
                \includegraphics[width=0.7\linewidth]{alexnet-softerror}
        }
        \qquad
        \subfloat[VGG-16]{
                \label{fig:vgg16}
                \includegraphics[width=0.7\linewidth]{vgg16-softerror}
        }
        \qquad
        \subfloat[VGG-19]{
                \label{fig:vgg19}
                \includegraphics[width=0.7\linewidth]{vgg19-softerror}
        }
        \caption{The precision accuracy of the benchmark neural network models on accelerators with different computing errors}
        \label{fig:softerror-accuracy}
\end{figure}


Comparing the second case and the third case, we find that the critical layer 
that suffers more computing errors can be viewed as the 'shortest 
wooden bar' of the overall neural network in terms of resilience. When it is protected, 
the overall neural network resilience gets improved significantly.
While scheduling the critical layers to GPPs may lead to additional computing overhead 
due to the computing gap between GPP and the accelerators, we need to evaluate the 
performance overhead. The relative performance of the second case and the third case 
is shown in Figure \ref{fig:clp_perf}. The performance penalty is less than two percent 
in the three neural networks. Considering the gains of relaxed design constraints, 
it is usually beneficial to schedule the small neural network computing layers to GPPs. 

\begin{figure}
        \center{\includegraphics[width=0.7\linewidth]{clp_time}}
        \caption{Relative runtime of neural networks when the critical layer is scheduled to CPU.}
        \label{fig:clp_perf}
\end{figure}

We decide the critical layers using the error distribution as shown in Figure \ref{fig:ber-error-distribute}.
We set the error threshold to be 5 and the experiment reveals that the last FC 
layer has the largest portion of computing errors that are more than 5. Thus, it is considered as 
the most critical layer. The critical layer takes only a small portion of the overall 
neural network computing, so the performance penalty is small even 
when it is scheduled to CPU. The last FC layer in AlexNet takes up higher portion of computing, 
the performance penalty is relatively higher compared to VGG16 and VGG19.
\begin{figure*}
        \center{\includegraphics[width=0.8\linewidth]{error_distribute_softerror}}
        \caption{Error distribution across the neural network layers when highest BER is used in AlexNet, VGG16 and VGG19.}
        \label{fig:ber-error-distribute}
\end{figure*}

