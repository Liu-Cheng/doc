\section{Experiments} \label{sec:casestudy}
In this section, we evaluate the proposed resilient neural network training 
framework for accelerators with computing errors. The errors can be caused by 
various relaxed design constraints and we use random computing errors in 
the experiments for general analysis. Then we use overclocking on FPGA based 
CNN accelerators as the case study and demonstrate the usefulness of the 
proposed resilient training on realistic system.

\subsection{Experiment setup}
We experiment on 8bit fixed-point PipeCNN \cite{pipecnn_2} accelerators on Xilinx KCU1500.
The FPGA board is attached to Intel(R) Core(TM) i7-6700 CPU @3.40GHz with 32GB memory.
To simulate general hardware errors caused by relaxed design constraints, we 
inject random bit errors to input/output data including 
input/intermediate/output features and weights as well as 
hidden layer status of neural networks. The error injection is measured with 
bit error rate (BER) which is also utilized in xxx. Compared to xxx,
we also have random errors injected to the internal computing results. 
To evaluate the training, we take three representative convolution 
neural networks including AlexNet, VGG-16 and VGG-19 as the benchmark. 
The neural network benchmark is summarized in Table \ref{tab:CNN-table}. 
The analysis can be applied to more neural networks.

\begin{table}[h]
        \centering
        \vspace{-0.3em}
        \caption{Neural network benchmark}
        \label{tab:CNN-table}
        \vspace{-0.3em}
        \begin{tabular}{c|c|c|c}
		\toprule
		  & Dataset & Layers & Total weights \\
		\midrule
		AlexNet & ImageNet & 8 & 61M \\
		\midrule
		VGG-16 & ImageNet & 16 & 138M \\
		\midrule
		VGG-19 & ImageNet & 19 & 143M \\
		\bottomrule
        \end{tabular}
        \vspace{-1em}
\end{table}

\subsection{Neural network resilience analysis}
To explore the resilience of the proposed neural network training, we
compare the prediction accuracy of neural networks in three scenarios.
In the first case, we have offline trained neural network models deployed on 
CNN accelerators with computing errors directly as denoted as 'original'.
In the second case, we have the neural network models retrained on the 
accelerator with computing errors. It is represented as training with 
accelerator (TWA). In the third case, we have the critical layers 
protected on top of the second case. Basically, we schedule the critical layers to 
GPPs to ensure precise computing during both retraining and inference.
It is denoted as critical layer protected(TWA+CLP).

The comparison of the three cases is presented in Figure \ref{fig:softerror-accuracy}.
When the BER goes up, the prediction accuracy of the original neural network drops 
considerably despite the resilience of the neural networks. 
With the proposed training i.e. TWA+CLP, the top1 and top5 precision accuracy 
of the retrained models improves by xxx and xxx on average respectively 
compared to the offline trained model under the highest error injection rate. 
The great prediction accuracy improvement indicates that the resilience 
of the retrained neural network models is improved targeting at the 
specific computing error pattern. Therefore, more aggressive design trade-offs 
between prediction accuracy and performance or energy efficiency can be performed. 

Comparing the second case and the third case, we find that the critical layer 
is sensitive to the computing errors and it can be considered as the 'shortest 
wooden bar' of the overall neural network in terms of resilience. When it is protected, 
the overall neural network resilience gets improved accordingly.
While scheduling the critical layers to GPPs may lead to additional computing overhead 
due to the computing gap between GPP and the accelerators, we need to evaluate the 
performance overhead. The relative performance of the second case and the third case 
is given in Figure \ref{fig:clp_perf}. The performance penalty is less than percent 
in the three neural networks. Considering the gains of relaxed design constraints, 
it is usually beneficial to schedule the small neural network computing layers to GPPs. 

\begin{figure}
        \center
		\subfloat[AlexNet]{
                \label{fig:alexnet}
                \includegraphics[width=0.7\linewidth]{alexnet-softerror}
        }
        \qquad
        \subfloat[VGG-16]{
                \label{fig:vgg16}
                \includegraphics[width=0.7\linewidth]{vgg16-softerror}
        }
        \qquad
        \subfloat[VGG-19]{
                \label{fig:vgg19}
                \includegraphics[width=0.7\linewidth]{vgg19-softerror}
        }
        \caption{The precision accuracy of the benchmark neural network models on accelerators with differnt computing errors}
        \label{fig:softerror-accuracy}
\end{figure}

\begin{figure}
        \center{\includegraphics[width=0.6\linewidth]{clp_time}}
        \caption{Relative runtime of neural networks when the critical layer is scheduled to CPU.}
        \label{fig:clp_perf}
\end{figure}

We decide the critical layers using the error distribution as shown in Figure xxx.
We set the error threshold as 5 and the experiment reveals that the last FC 
layer has most large portion of computing errors and it is considered as 
the most critical layer. The last layer takes only a small portion of the overall 
neural network computing, so the performance penalty is small even 
when it is scheduled to CPU. The last FC layer in AlexNet takes up higher portion of computing, 
the performance penalty is relatively higher compared to VGG16 and VGG19.
\begin{figure*}
        \center{\includegraphics[width=0.85\linewidth]{error_distribute_softerror}}
        \caption{Error distribution across the neural network layers when highest BER is used in AlexNet, VGG16 and VGG19.}
        \label{fig:clp_perf}
\end{figure*}


\subsection{Overclocking case study}
To verify the proposed resilient training, we take overclocking on KCU1500 as a case 
study in this work. By relaxing the timing constraints, the CNN accelerator can 
run at higher clock frequency with timing violations and computing errors. In pipeCNN, the 
hardware implemention is related to the neural network. For AlexNet, VGG16 and VGG19, the 
default implemention frequency is 210 MHz, 190 MHz and 190 MHz respectively. We then apply overcloking 
on the implementions with a step of 10 MHz. The implementions can be boosted to 
260 MHz MHz, 240 MHz MHz and 240 MHz at most. By retraining the original neural network with 
the proposed approach, the prediction accuracy improvement is evaluated in detail.

Clock frequency is almost proportional to the performance of the CNN accelerator 
especially for large convolution operation which is typically computing bound. 
However, the circuit design tools typically adopt conservative design options 
in order to avoid the timing violations in the worst case. Overclocking enables 
higher clock frequency and performance, but timing errors may happen and affect 
the inference accuracy. The dynamic timing error caused by overclocking 
can not be captured by the offline training. In this case, we can also 
apply the on-accelerator training to have the computing errors tolerated 
by the models without redesigning the CNN accelerator.
\begin{figure}
        \center
	\subfloat[AlexNet]{
                \label{fig:alexnet}
                \includegraphics[width=0.6\linewidth]{alexnet-overclock}
        }
	\qquad
	\subfloat[VGG-16]{
                \label{fig:vgg16}
                \includegraphics[width=0.6\linewidth]{vgg16-overclock}
        }
        \qquad
	\subfloat[VGG-19]{
                \label{fig:vgg19}
                \includegraphics[width=0.6\linewidth]{vgg19-overclock}
        }
	\caption{The prediction accuracy of the benchmark neural networks on accelerators with different overclocking}
        \label{fig:overclock-accuracy}
\end{figure}


On top of the overclocked CNN accelerators, we compare both the offline 
training and the on-accelerator training.
The prediction accuracy comparison is presented in Figure \ref{fig:overclock-accuracy}. 
Basically, the prediction accuracy will not drop until the tipping point. After the tipping point, the 
prediction accuracy degrades dramatically and can no longer be salavaged through training.
While at the extreme overclocking frequency, the on-accelerator training exhibits clear 
improvemenmt on the benchmark. Compared to the offline training, 
the on-accelerator training improves the top1 and top5 prediction accuracy 
on the extreme overclocking case by 13.7\% and 11.6\% respectively.
Compared to the original design, the clock frequency 
increases by 19\% to 26\% with small prediction accuracy loss. 
LeNet again has distinct behavior. It can tolerate all the
overclocking errors before the tipping point, but it 
also crashed given higher clock.

\subsection{Proposed training analysis}
For the proposed training approach, we notice that batch size is 
a critical hyper parameter. We use AlexNet as an example and show the influence of batch on the resulting model 
prediction accuracy. The result is presented in Figure \ref{fig:batch}.
According to the figure, the batch size affects the training precision clearly.
Given larger batch size, the model prediction accuracy 
increase gradually. The main reason is that larger batch training helps to 
reduce the influence of the computing error. Therefore, larger batch size is 
recommended for the on-accelerator training.

\begin{figure}
        \center{\includegraphics[width=0.6\linewidth]{batch}}
        \caption{The influence of batch size on the on-accelerator training}
        \label{fig:batch}
\end{figure}

Comparing the experiments with random error simulation and overclocking on realistic FPGAs, we
find that the computing errors may not be uniform distributed. In the overclocking, it can be found 
that the prediction accuracy drops with clif-like style. It indicates the computing errors may 
be quite small with light overclocking, but the amount of errors explodes at certain point.
Fortunately, the proposed training framework still functions as expected.

