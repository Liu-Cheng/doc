\section{Experiments} \label{sec:casestudy}
In this section, we evaluate the proposed resilinet neural network training 
framework for accelerators with computing errors. The errors can be caused by 
various relaxed design constraints and we use random computing errors in 
the experiments for general analysis. Then we use overclocking on FPGA based 
CNN accelerators as the case study and demonstrate the usefulness of the 
proposed resilient training on realistic system.

\subsection{Experiment setup}
We experiment on 8bit fixed-point PipeCNN \cite{pipecnn_2} accelerators. 
To simulate the computing errors caused by related design constraints, we 
inject random errors to on-chip buffers and hidden layer status registers of 
the CNN accelerators which is similar to the error simulation used in xxx. 
The main difference is that we also have random errors injected to the 
internal computing results as suggested in xxx. The error injection metric 
is called bit error rate which represents xxx. To evaluate the training, 
we take four representative convolution neural networks including 
AlexNet, VGG-16 and VGG-19 as the benchmark. The neural network 
benchmark is summarized in Table \ref{tab:CNN-table}. It covers networks with 
diverse sizes and the analysis can be applied to more neural networks.
\begin{table}[h]
        \centering
        \vspace{-0.3em}
        \caption{Neural network benchmark}
        \label{tab:CNN-table}
        \vspace{-0.3em}
        \begin{tabular}{c|c|c|c}
		\toprule
		  & Dataset & Layers & Total weights \\
		\midrule
		AlexNet & ImageNet & 8 & 61M \\
		\midrule
		VGG-16 & ImageNet & 16 & 138M \\
		\midrule
		VGG-19 & ImageNet & 19 & 143M \\
		\bottomrule
        \end{tabular}
        \vspace{-1em}
\end{table}

\subsection{Neural network resilience analysis}
To explore the resilience of the proposed neural network training, we
compare the prediction accuracy of neural networks in three scenarios.
In the first case, we have offline trained neural network models deployed on 
CNN accelerators with computing errors directly as denoted as 'original'.
In the second case, we have the neural network models retrained on the 
accelerator with computing errors. It is represented as training with 
accelerator (TWA). In the third case, we have the critical layers 
protected on top of the second case and it is denoted as critical layer protected(CLP). 
Basically, we schedule the critical layers to 
GPPs to ensure precise computing during both retraining and inference.

The comparison of the three cases is presented in Figure \ref{fig:softerror-accuracy}.
When the BER goes up, the prediction accuracy of the original neural network drops 
considerably despite the resilience of the neural networks. 
According to the experiments, the top1 and top5 precision accuracy of the retrained models 
improves by 11.1\% and 3.4\% on average respectively compared to the offline trained 
model under the highest error injection rate. By using the proposed training, 
the prediction accuracy recovers significantly, which indicates that the resilience 
of the neural network models is improved. Therefore, more aggressive design trade-offs 
between prediction accuracy and performance or energy efficiency can be performed. 

Comparing the second case and the third case, we find that the critical layer 
is sensitive to the computing errors and it can be considered as the 'shortest 
wooden bar' of the overall nerual network resilience. When it is protected, 
the overall neural network resilience gets improved accordingly.
While scheduling the critical layers to GPPs may lead to additional computing overhead 
due to the comptuing gap between GPP and the accelerators, we need to evaluate the 
performance overhead. According to our experiemnts, the last layers are prone to 
be the critical layers which usually take only a small portition of the overall 
computing overhead. For instance, the computing overhead is only xxx of the overall computing.
Assume the accelerator is 100X faster, then the performance penalty is around xxx.
When the improved prediction accuracy allows xx, it is benefitical to schedule the 
last computing layer to GPPs. When there is additional precise CNN accelerators shared by 
accelerator with computing errors, it is always benefical to protect the critical computing layers.

\begin{figure}
        \center
		\subfloat[AlexNet]{
                \label{fig:alexnet}
                \includegraphics[width=0.6\linewidth]{alexnet-softerror}
        }
        \qquad
        \subfloat[VGG-16]{
                \label{fig:vgg16}
                \includegraphics[width=0.6\linewidth]{vgg16-softerror}
        }
        \qquad
        \subfloat[VGG-19]{
                \label{fig:vgg19}
                \includegraphics[width=0.6\linewidth]{vgg19-softerror}
        }
        \caption{The precision accuracy of the benchmark neural network models on accelerators with differnt computing errors}
        \label{fig:softerror-accuracy}
\end{figure}

The computing errors in the worst case of the three neural networks are also evaluated as shown in 
Figure xxx. We set the error threshold as 5, it can be seen that the last FC layer is the most 
critical layer according to this metric. 

\subsection{Overclocking case study}
To verify the proposed resilient training, we take overclocking on KCU1500 as a case 
study in this work. By relaxing the timing constraints, the CNN accelerator can 
run at higher clock frequency with timing violations and computing errors. In pipeCNN, the 
hardware implemention is related to the neural network. For AlexNet, VGG16 and VGG19, the 
default implemention frequency is 210 MHz, 190 MHz and 190 MHz respectively. We then apply overcloking 
on the implementions with a step of 10 MHz. The implementions can be boosted to 
260 MHz MHz, 240 MHz MHz and 240 MHz at most. By retraining the original neural network with 
the proposed approach, the prediction accuracy improvement is evaluated in detail.

Clock frequency is almost proportional to the performance of the CNN accelerator 
especially for large convolution operation which is typically computing bound. 
However, the circuit design tools typically adopt conservative design options 
in order to avoid the timing violations in the worst case. Overclocking enables 
higher clock frequency and performance, but timing errors may happen and affect 
the inference accuracy. The dynamic timing error caused by overclocking 
can not be captured by the offline training. In this case, we can also 
apply the on-accelerator training to have the computing errors tolerated 
by the models without redesigning the CNN accelerator.
\begin{figure}
        \center
	\subfloat[AlexNet]{
                \label{fig:alexnet}
                \includegraphics[width=0.6\linewidth]{alexnet-overclock}
        }
	\qquad
	\subfloat[VGG-16]{
                \label{fig:vgg16}
                \includegraphics[width=0.6\linewidth]{vgg16-overclock}
        }
        \qquad
	\subfloat[VGG-19]{
                \label{fig:vgg19}
                \includegraphics[width=0.6\linewidth]{vgg19-overclock}
        }
	\caption{The prediction accuracy of the benchmark neural networks on accelerators with different overclocking}
        \label{fig:overclock-accuracy}
\end{figure}


On top of the overclocked CNN accelerators, we compare both the offline 
training and the on-accelerator training.
The prediction accuracy comparison is presented in Figure \ref{fig:overclock-accuracy}. 
Basically, the prediction accuracy will not drop until the tipping point. After the tipping point, the 
prediction accuracy degrades dramatically and can no longer be salavaged through training.
While at the extreme overclocking frequency, the on-accelerator training exhibits clear 
improvemenmt on the benchmark. Compared to the offline training, 
the on-accelerator training improves the top1 and top5 prediction accuracy 
on the extreme overclocking case by 13.7\% and 11.6\% respectively.
Compared to the original design, the clock frequency 
increases by 19\% to 26\% with small prediction accuracy loss. 
LeNet again has distinct behavior. It can tolerate all the
overclocking errors before the tipping point, but it 
also crashed given higher clock.

\subsection{Proposed training analysis}
For the proposed training approach, we notice that batch size is 
a critical hyper parameter. We use AlexNet as an example and show the influence of batch on the resulting model 
prediction accuracy. The result is presented in Figure \ref{fig:batch}.
According to the figure, the batch size affects the training precision clearly.
Given larger batch size, the model prediction accuracy 
increase gradually. The main reason is that larger batch training helps to 
reduce the influence of the computing error. Therefore, larger batch size is 
recommended for the on-accelerator training.

\begin{figure}
        \center{\includegraphics[width=0.6\linewidth]{batch}}
        \caption{The influence of batch size on the on-accelerator training}
        \label{fig:batch}
\end{figure}

Comparing the experiments with random error simulation and overclocking on realistic FPGAs, we
find that the computing errors may not be uniform distributed. In the overclocking, it can be found 
that the prediction accuracy drops with clif-like style. It indicates the computing errors may 
be quite small with light overclocking, but the amount of errors explodes at certain point.
Fortunately, the proposed training framework still functions as expected.

