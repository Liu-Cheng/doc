\section{Motivation} \label{sec:motivation}
Neural networks are usually resilient to computing errors. With this feature, 
significant improvements on performance or energy efficiency may be 
achieved with moderate computing errors but small prediction accuracy loss.
Therefore, more resilient neural networks models promise more advantageous 
design trade-offs. To improve neural network model resilience, an  approach is to adopt 
the approaches such as training with noisy data and dropout, which are mainly 
used to reduce the influence of sampling variations and prevent overfitting. 
In the rest of this section, we take overclocking as an example of accelerators 
with computing errors, analyze the influence of computing errors on 
the neural network model prediction accuracy and evaluate the resilience of 
more general neural network models.

In the experiment, we use PipeCNN\cite{pipecnn_2}, an open sourced high-level CNN accelerator, 
as the baseline design. We have it implemented on KCU1500 FPGA board and run at 210 MHz. 
On ImageNet, we train AlexNet, VGG16 and VGG19 offline and then apply them to the 8bit accelerator. 
When the accelerator is overclocked and the clock is boosted to the extreme case where 
further overclocking will lead to frequent hardware crash, we measure the prediction accuracy of 
the three typical neural networks and compare with that obtained from accelerators with normal 
clock. In addition, we also evaluate the models trained with noisy data and dropout and 
compare with the original model.

Figure \ref{fig:top1-loss} and Figure \ref{fig:top5-loss} show the top1 and top5 prediction accuracy comparison 
respectively. 'Overclocking' refers to the neural networks models running on overclocked accelerator. 
'train with dropout' and 'train with noise' represent the models trained with dropout strategy 
and noisy data respectively. 'Original' means the models running on normal accelerator.
It can be seen that all the three neural network models suffer considerable accuracy loss 
when the models are deployed on the overclocked accelerator directly. The average top1 accuracy and top5 accuracy 
drops by xxx and xxx respectively, which undermines the usefulness of overclocking dramatically.
Particularly, top1 accuracy drops more than top5 accuracy on all the 
models, which means that the top1 of the final softmax output changes but it probably 
stays in the top5 range. 

We also evaluated the models trained with noisy data and dropout. 
The comparison reveals that offline training with dropout can hardly 
improve the resilience of the neural network models and even lowers the 
prediction accuracy slightly when the models are executed on the 
overclocked accelerator. While neural network models trained with 
noisy data have distinct influence on different models, the 
prediction accuracy of AlexNet drops but the accuracy of VGG16 
improves. The prediction accuracy of VGG19 does not change much.
It indicates that computing errors caused by the noisy data are 
different from that incurred by overclocking. The difference varies 
in different neural network models and there is no guarantee 
for the accuracy improvement using noisy data training.

\begin{figure}
	\center{\includegraphics[width=0.75\linewidth]{top1_dropout_noisy}}
    \caption{Top1 prediction accuracy loss of models running on overclocked CNN accelerators.}
\label{fig:top1-loss}
\vspace{-1em}
\end{figure}

\begin{figure}
        \center{\includegraphics[width=0.75\linewidth]{top5_dropout_noisy}}
    \caption{Top5 prediction accuracy loss of models running on overclocked CNN accelerators.}
\label{fig:top5-loss}
\vspace{-1em}
\end{figure}

To gain further insight of the computing errors, we analyze the error distribution 
of different layers. Based on the error range, we divide the errors into 11 categories 
including R0 ($0 \leq error < 1$), R1 ($1 \leq error < 2$), ..., R9 ($9 \leq error < 10$) 
and R10 ($error \geq 10$). The error distribution of the three neural networks is shown in 
Figure xxx. Errors on the different layers also vary. The last layer which is close to the result 
exhibits the more large errors. 

According to the above experiments, we observe that the computing errors induced by the relaxed 
design constraints such as overclocking can lead to dramatic prediction accuracy loss, though 
the neural networks are usually resilient. The errors can be rather difficult to be characterized 
via offline training with either noisy data or dropout. 
Instead of training on GPPs, we try to integrate the accelerator in the training framework 
such that computing errors can be considered during training directly. In addition, we notice 
that the last layer typically suffer more large errors, so we opt to protect the last layer 
that takes up only a small portion of the neural network computing by scheduling it to GPPs 
or other infrastructures without errors to improve the resilience of the overall neural 
networks and obtain more advantageous design trade-offs.

