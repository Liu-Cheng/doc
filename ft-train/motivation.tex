\section{Motivation} \label{sec:motivation}
Neural networks are usually resilient to computing errors. With this feature, 
significant improvements on performance or energy efficiency may be 
achieved with moderate computing errors but small prediction accuracy loss.
Therefore, more resilient neural networks models promise more advantageous 
design trade-offs. To improve neural network model resilience, an  approach is to adopt 
the approaches such as training with noisy data and dropout, which are mainly 
used to reduce the influence of sampling variations and prevent overfitting. 
In the rest of this section, we take overclocking as an example of accelerators 
with computing errors, analyze the influence of computing errors on 
the neural network model prediction accuracy and evaluate the resilience of 
more general neural network models.

In the experiment, we use PipeCNN\cite{pipecnn_2}, an open sourced high-level CNN accelerator, 
as the baseline design. We have it implemented on KCU1500 FPGA board. 
On ImageNet, we train AlexNet, VGG16 and VGG19 offline and then apply them to the 8bit accelerator. 
When the accelerator is overclocked and the clock is boosted to the extreme case where 
further overclocking will lead to frequent hardware crash, we measure the prediction accuracy of 
the three typical neural networks and compare with that obtained from accelerators with normal 
clock. In addition, we also evaluate the models trained with noisy data and dropout and 
compare with the original model.

Figure \ref{fig:top1-loss} and Figure \ref{fig:top5-loss} show the top1 and top5 prediction accuracy comparison 
respectively. It can be found that all the three neural network models suffer considerable accuracy loss 
when the models are deployed on the overclocked accelerator directly. The average top1 accuracy and top5 accuracy 
drops by xxx and xxx respectively, which undermines the usefulness of overclocking dramatically.
Particularly, top1 accuracy drops more than top5 accuracy on all the 
models, which means that the top1 of the final softmax output changes but it probably 
stays in the top5 range. 

We also evaluated the models trained with noisy data and dropout. 
The comparison reveals that offline training with dropout can hardly 
improve the resilience of the neural network models and even lowers the 
prediction accuracy slightly when the models are executed on the 
overclocked accelerator. While neural network models trained with 
noisy data have distinct influence on different models, the 
prediction accuracy of AlexNet drops but the accuracy of VGG16 
improves. The prediction accuracy of VGG19 does not change much.
It indicates that computing errors caused by the noisy data are 
different from that incurred by overclocking. The difference varies 
in different neural network models and there is no guarantee 
for the accuracy improvement using noisy data training.
\begin{figure}
        \center
        \subfloat[top1 prediction accuracy]{
                \label{fig:top1-loss}
                \includegraphics[width=0.7\linewidth]{top1_dropout_noisy}
        }
        \qquad
        \subfloat[top5 prediction accuracy]{
                \label{fig:top5-loss}
                \includegraphics[width=0.7\linewidth]{top5_dropout_noisy}
        }
        \caption{Prediction accuracy loss of models running on overclocked CNN accelerators.
			'Overclocking' refers to the neural networks models running on overclocked accelerators. 
'train with dropout' and 'train with noise' represent the models trained with dropout strategy 
and noisy data respectively. They are also applied on overclocked accelerators. 
'Original' means the models running on normal accelerator.
 }
 \vspace{-1em}
        \label{fig:accuracy-dropout-noisy}
\end{figure}

To gain further insight of the computing errors, we analyze the error distribution 
of different layers when compared to the correct execution. Based on the error range, 
we divide the errors into 3 categories including R0 ($error = 0$), 
R1 ($0 \leq error \leq 5$) and R2 ($error > 5$). As shown in 
Figure \ref{fig:error_distribute}, most of the computing results remain 
correct even in the extreme overclocking scenario. Full connection (FC) in the last layer 
has only a small portion of correct computing results and most of the computing errors are 
larger than xxx. As the last layer is close to the result and affects the final output 
directly, it becomes critical to improve the overall resilience of the neural 
networks. 

\begin{figure*}
        \center{\includegraphics[width=0.85\linewidth]{error_distribute}}
        \caption{Computing error distribution across the different layers}
        \label{fig:error_distribute}
\end{figure*}

According to the above experiments, we observe that the computing errors induced by 
overclocking can lead to dramatic prediction accuracy loss, though 
the neural networks are usually resilient. The errors are rather difficult to be
covered via offline training with noisy data and dropout. 
Instead of training on GPPs, we try to integrate the accelerator in the training framework 
such that computing errors can be considered during training directly. In addition, we notice 
that the last layer typically suffer more large errors, so we opt to protect the last layer 
that takes up only a small portion of the neural network computing. By scheduling it to GPPs 
or other infrastructures without errors may improve the resilience of the overall neural 
networks, which allows more advantageous design trade-offs.

