\section{Related Work} \label{sec:relatedwork}
In this section, we will review the neural network training for CNN accelerators first.
Then we will discuss the various scenarios that CNN accelerator 
may produce computing with variation, which may lead to 
considerable network prediction accuracy loss and can not be handled 
with the offline training.

\subsection{Training for CNN accelerators} 
Numerous hardware accelerators especially the CNN accelerators have
been developed in the past few years. \cite{Cnvlutin_25} 
\cite{Caffeine_6,deepburing_12}, \cite{Aydonat_27}, 
\cite{Caffeine_6}, \cite{Caffeine_6,Aydonat_27}, \cite{Wei_29}. To 
deploy the neural network models on the accelerators efficiently, 
the models need to be trained specifically for the target accelerator. 
The neural network training is mostly performed on GPPs and
floating point models are widely used. While the CNN accelerators are usually fixed point, 
a widely used training approach is to add fixed point constraint to the training. 
The training is widely used for neural network model quantization and it is 
done on GPPs\cite{Hwang2014_17, Matthieu2014_8, Hwang2014_17}. 

There are also efforts spent to use FPGAs for training. 
Caffeine\cite{Caffeine_6} provides a general CNN accelerator design and integrates it with 
the training framework Caffe. Caffeinated FPGA\cite{DiCecco_4} implemented FPGA kernels 
for both forward and backward. It can also be used in Caffe and allows for 
both training and inference. These work focus on the performance optimization 
and they do not take the computing variation of the CNN accelerators into consideration.

\subsection{Computing variation of CNN accelerators} 
The computing on CNN accelerators may vary due to various reasons 
such as approximate arithmetic operations, overclocking, soft error.
Overclocking is a technique to increase the operating speed, though 
it may cause timing errors. It has been explored in many hardware 
designs \cite{overclock_3}, \cite{Razor}. 
Soft errors are transitions of logic state in a circuit 
which is usually caused by external source of ionizing radiations. 
It increases with the shrinking transistor sizes. 
Approximate computing is a promising technique for 
energy-efficiency optimization\cite{Miao_40,han_41}.
Among the approximate computing techniques, approximate 
arithmetic operations \cite{appro_45,approxANN_44} can be a good fit to the CNN accelerator 
which involves large amount of arithmetic operations. 
In summary, the computing in all the above scenario may vary dynamically, 
the offline trained model can not be deployed on the 
accelerators in these scenarios. They should be considered in the 
neural network training. 

