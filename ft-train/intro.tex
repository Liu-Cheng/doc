\section{Introduction} \label{sec:intro}
Inspired by the widespread adoption of neural networks in massive fields, neural network accelerators 
\cite{deepburing_12,DiCecco_4} 
are increasingly explored and deployed to improve the computing performance and energy efficiency.
Unlike generic applications, neural networks usually involve redundancy and are known to be 
fault tolerant\cite{Reagen2016}. By taking advantage of this feature, many neural network accelerator optimizations 
such as neural network pruning and quantization can be utilized to improve 
performance and energy efficiency notably with minor inference accuracy penalty\cite{Han2016DeepCC}. 

Following similar ideas to compromise between neural network accuracy and performance, 
we opt to relax the design constraints of the neural network accelerators 
for significant performance or energy efficiency improvements with 
minor prediction accuracy loss. In this case, many aggressive hardware 
optimization techniques can be applied when computing errors can 
be tolerated. For instance, emerging techniques such as
near-threshold voltage regime\cite{RG2010NT} and sub-threshold digital
logic design\cite{BH2005} promise high energy efficiency but suffer
instability\cite{Pu2010NT}. Conventional neural network accelerator can
be pushed to operate at higher clock frequency with timing
violations\cite{overclock_3}. They will bring benefits to the accelerator design 
on various aspects including performance and energy efficiency.

Motivated by the great advantages of relaxed design constraints,
we further explore the use of neural network resilience
for more effective design trade-offs. Instead of deploying the 
unmodified neural network models on the accelerators directly, 
we borrow the retraining idea from prior neural network 
quantization work\cite{Hwang2014_17} and have the deep neural network models to 
learn and tolerate the computing errors.
Basically, we have the forward computing
performed on the accelerator and then transfer the computing
results to the host processor for backward propagation. With
this approach, both the application data and computing errors
are learned and incorporated in the neural network models.
Meanwhile, we define a set of standard interfaces to make
it convenient to integrate general CNN accelerators into the
retraining framework.

In addition, we notice that some of the neural network layers
are more sensitive to the computing errors and the sensitive
layers dramatically limit the usefulness of the retraining. Thus,
we schedule the most sensitive layer of the neural networks
to host processors to reduce the negative influence of the
computing errors. With both the retraining and sensitive layer
protection, the neural networks become more resilient to the
computing errors caused by the aggressive design options
such as near-threshold logic or overclocking. Compared to the
original neural networks, the top1 and top5 prediction accuracy
improves by 20.7\% and 5.9\% on average according to the
experiments.

%\begin{itemize}
%	\item We propose to improve the fault-tolerance of neural networks and make use of it to relax 
%		the accelerator design constraints for higher performance or energy efficiency.
%
%	\item We propose a neural network training framework to obtain resilient neural network models. 
%		By integrating the accelerators into conventional training, we have the computing errors 
%		learned with the application data. By protecting the most fragile network layer, 
%		we can further improve the resilience of the neural networks and pose more 
%		opportunities to hardware optimizations.
%
%	\item With comprehensive experiments, we show that the proposed training framework 
%		could enhance the prediction accuracy of neural networks significantly 
%		when the accelerators run with computing errors.
%\end{itemize}
%The paper is organized as follows. Section II analyzes the influence of 
%the CNN accelerator computing errors on the neural network prediction accuracy. 
%Section III presents the proposed resilient neural network training on accelerators with computing errors.
%Section IV demonstrates the use of the training on accelerators with computing errors. 
%Section V briefs the related work and Section VI draws the conclusion. 
%

