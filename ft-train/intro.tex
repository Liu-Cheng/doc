\section{Introduction} \label{sec:intro}
Inspired by the widespread adoption of neural networks in massive fields such as image classification, 
video surveillance, speech recognition, and robot vision, neural network accelerators 
\cite{deepburing_12,DiCecco_4} 
are increasingly explored and deployed to improve the computing performance and energy efficiency.
Unlike generic applications, neural networks usually involve redundancy and are known to be 
fault tolerant\cite{Reagen2016}. By taking advantage of this feature, many neural network accelerator optimizations 
such as neural network pruning and low-precision quantization can be utilized to improve 
performance and energy efficiency notably with minor inference accuracy penalty\cite{Han2016DeepCC}. 

In line with these optimizations, we can relax the design constraints of 
the neural network accelerators to obtain significant performance or energy efficiency 
improvements with minor prediction accuracy loss. 
For generic hardware design, strict design constraints are typically required to 
guarantee correct computing under even the most severe environments. On the contrast, 
neural network accelerators are more resilient and less sensitive to computing 
errors incurred by relaxed design constraints. When there are computing errors in the accelerator, 
which may cause clear prediction accuracy loss. We borrow the retraining 
idea from prior neural network quantization work \cite{Hwang2014_17} 
and have the deep neural network models to learn and tolerate the computing errors.  
Basically, we have the forward computing performed on the accelerator and 
then transfer the computing results to the host processor for 
backward propagation. With this approach, both application data and computing 
errors are learned and incorporated in the neural network models.  

In addition, we notice that some of the neural network layers are more sensitive to the 
computing errors and the sensitive layers dramatically limit the usefulness of the retraining. 
Thus, we schedule the most sensitive layer of the neural networks to host processors to reduce the negative influence 
of the computing errors. With both the retraining and sensitive layer protection, 
the neural networks become more resilient to the computing errors caused by 
the aggressive design options such as near-threshold logic or overclocking.
Compared to the original neural networks, the top1 and top5 prediction accuracy
improves by 20.7\% and 5.9\% on average according to the experiments. 

%\begin{itemize}
%	\item We propose to improve the fault-tolerance of neural networks and make use of it to relax 
%		the accelerator design constraints for higher performance or energy efficiency.
%
%	\item We propose a neural network training framework to obtain resilient neural network models. 
%		By integrating the accelerators into conventional training, we have the computing errors 
%		learned with the application data. By protecting the most fragile network layer, 
%		we can further improve the resilience of the neural networks and pose more 
%		opportunities to hardware optimizations.
%
%	\item With comprehensive experiments, we show that the proposed training framework 
%		could enhance the prediction accuracy of neural networks significantly 
%		when the accelerators run with computing errors.
%\end{itemize}
%The paper is organized as follows. Section II analyzes the influence of 
%the CNN accelerator computing errors on the neural network prediction accuracy. 
%Section III presents the proposed resilient neural network training on accelerators with computing errors.
%Section IV demonstrates the use of the training on accelerators with computing errors. 
%Section V briefs the related work and Section VI draws the conclusion. 
%

