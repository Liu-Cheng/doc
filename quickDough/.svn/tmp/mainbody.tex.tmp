\section{Proposed Design Methodlogy}\label{fig:methodology}
The proposed SCGRA based HLS design methodology as shown in Figure \ref{fig:design-methodology} has two layers: the per application domain layer and the per application layer. The per application domain layer abstracts the common computation kernel characteristics such as computation types, scales and patterns, and then it refines the hardware specifications including the SCGRA scale, PE configuration, and IO bandwidth according to these characteristics. Since the SCGRA structure varies little with applications, rough hardware specifications are able to limit the SCGRA implementation to a few occassions or even determine the SCGRA implementation. In this case, all the possible SCGRA implementations can be prepared in advance without targeting at any specific application. In this work, the basic SCGRA structure is determined when the required computation types are known. Different applications that employ the basic SCGRA structure only differs in instruction memory capacity. Therefore, we are able to cover an application domain by simply implementing the SCGRA with a series of instruction memory capacity. The per application layer targets at the implementation of a specific application. It abstracts IR from HLL program using LLVM \cite{llvm} first. Then it schedules the IR to a pre-implemented SCGRA using a delicate list scheduling algorithm. At the end of the scheduling, the control words that dictate all the activities of the SCGRA are dumped. (Since the control words are used to control the SCGRA cycle by cycle, we also call the control words instructions.) When all the instructions are collected, they are sent to the Xilinx tool data2mem \cite{data2mem} together with the pre-implemented bitstream. Thanks to this ISE independent tool, the instruction memory context of the pre-implmented SCGRA bitstream can be replaced with the new instructions directly and a new bitstream is generated. The new bitstream is actually an implementation of the application and can be downloaded to FPGA. Details of the design methodology will be elaborated in the following sections.


\begin{figure*}[htb]
\centering
\includegraphics[width=12cm]{designflow}
\caption{The proposed SCGRA based HLS design methodology}
\label{fig:design-methodology}
\end{figure*}


\subsection{IR Abstraction}
The proposed HLS methodology mainly targets at accelerating the computation intensive kernels using FPGA. Typically the computation kernels can be loops with large iterations or functions called repeatedly and it is not applicable to deploy an extensively expanded kernel to FPGA due to the resource constrain and the IO constrain. Meanwhile, theprimitive computation kernel body can be light weight and its parallelism is insufficient for FPGA acceleration. In this case, a computation kernel can be appropriately reshaped using techniques such as partial unrolling, such that the entire kernel can be divied into a number of identical kernel bodies and the computation kernel can be accelerated by simply deploying the kernel body to FPGA. When the kernel bodies are independent with each other, arbitrary number of kernel bodies can be put together and the reshaping is pretty simple. When the kernel bodies are dependent with each other, the reshaping problem gets complex. A preliminary solution to this peroblem is based on modulo scheduling \cite{rau1994iterative}. With modulo scheduling, the operations of the computation kernel can be piplined. By ignoring the prologue part and epilogue part of the pipeline, the main part of the pipeline can be viewed as a group of identical computation bodies with different scales. Consequently, the computation kernel with dependent kernel bodies can also be reshaped as required. The reshaping scheme is not the foucs of the paper and extensive efforts are still needed to figure out an optimal solution, sowe leave this for future study. In this paper, we assume that we are accelerating an computation kernel body. Also note that the term 'computation kernel body' is regarded as 'computation kernel' for the sake of convenience in the following sections. 


The computation kernel may have inner loops, branches or sub functions. To fully explore the parallelism of the kernels, all the loops are completely unrolled, all the sub functions are inlined and all the branches are merged. After this step, the kernel turns to be a basic block and it can be further compiled to DFG. DFG describes the data dependency of the HLL program, and helps explore operation parallelism during the SCGRA scheduling step especially for these computation kernels. Hence, it is considered as IR of our HLS methodology. In this paper, the computation kernel that can be deployed to FPGA is manually identified. When the computation kernel is specified, it is then compiled to the machine-independent assembly language of LLVM through Clang \cite{llvm}. Moreover, LLVM passes such as dead code elimination, loop simplify, loop unroll, and function inline are employed for code optimization. At the same time, branch instructions are removed by introduing the PHI instruction \cite{llvm}. Finally, the code turns to be static single assignment (SSA) style. Taking the SSA code as input, we deliver it to our own pass and transform the LLVM assemble code to a DFG. Meanwhile,note that the IR abstraction takes LLVM as a compiling platform, so any HLL supported by LLVM front-end can be input of the proposed HLS methodology.


\subsection{SCGRA Scheduling}
In the SCGRA scheduling step, a list scheduling algorithm \cite{schutten1996list} is adopted to schedule the DFG to the SCGRA infrastructure. The basic list scheduling process is simple. Initially, it constructs an operation list in which all the operations have their source operands ready. Then it goes to the scheduling kernel. In the scheduling kernel, the PE that meets the PE selection metric is selected first. After that, the operation that fits the selected PE best according to the operation selection metric is chosen. As both the PE and the operation are determined, it gets to commit the scheduling, which includes finding the shortest routing paths, moving source operands to target PE along the routing paths and calculating the operation on target PE. Finally, it updates the operation list and repeats the scheduling kernel until the list becomes empty. In this work, we have made a few modifications on PE selection metric and operation selection metric to adapt to the DFG characteristics and the target CGRA structure.


Since each node of the DFG represents an operation and the DFG is actually a fine-granularity task graph, the PEs have much less time slots idle and can be busy all through the scheduling. In this case, we simply set the time interval that PE has been idle as the PE selection metric to find out the PE that guarantees the earliest computation. In addition, we also add PE utilization constrain to narrow down the PE candidates and help the scheduler to keep load balance. At the same time, we notice that the operation number of the DFG typically is much larger than the PE number of the SCGRA. There are sufficient ready operations in the DFG for scheduling and it makes little difference whether the operations on critical paths are executed earlier or later. At the same time, the routing cost between PEs is large due to the deep SCGRA pipeline depth, and routing distance becomes essential to the scheduling performance. Consequently, we set the routing cost that it takes the scheduler to move all the source operands to target PE as operation selection metric. Moreover, we also keep all the operand storing records, and each operand may have multiple copies across the CGRA. When an operand is needed for computation, we always fetch the source operand from the nearest PEs to further reduce the routing cost.

In order to move an source operand to the selected PE, a routing path for actually scheduling the hardware resources and corresponding time slots is needed. As the SCGRA status is changing through the scheduling process, a determined routing will soon deteriorates the scheduling performance. In this work, we take the time interval that it costs to move an operand from upstream PE data memory to downstream PE data memory as the link weight. Link weight is updated in real time and thus it indicates the routing congestion information of each link. With all the link weight across the SCGRA updated, we further calculate the routing path of a PE pair using Dijkstra algorithm. Apparently Dijkstra algorithm here takes both the routing distance and routing congestion information into account. Therefore, it helps to find out a near optimal routing in real time and is able to provide a fast operation transmission.

On top of the issues mentioned, the scheduler should also be responsible for the data memory management. The data memory management mainly involves two aspects. On the one side, it slightly adjusts the operation selection to reduce the intermediate operands that need to be stored in data memory and to make sure that the data memory does not overflow. In this work, we add an operation selection filter, which removes the operations with larger children operations from operation ready list, to control the data memory requirements. Note that this scheme is not able to precisely adapt the data memory consumption and it also influences the scheduling performance. Fortunately, the experiments show that the primitive data memory capacity is sufficient to fulfill the requirements of the computation kernels even without stringent filtering. On the other side, the data memory manager also needs to generate read/write addresses which are part of the instructions. Since the data memory requirements are not pressing, a static address generation is adopted. It does not start until the scheduling kernel is completed, so both an operand's read time $\left( tr_1, tr_2,...,tr_m \right)$ and write time $\left( tw_1,tw_2,...,tw_n \right)$ on a data memory can be obtained from scheduling records. Note that $tr_1<tr_2<...<tr_m$ and $tw_1<tw_2<...<tw_n$. In most occasions, $n=1$, but $n \geq 1$ sometimes because the routing path is changing all the time. An address is allocated to the operand at the first time it is stored in the data memory and the address is released at the last time it is referenced through the data memory. Therefore, the life time of the operand in the data memory is from $tw_1$ to $tr_m$.


\subsection{Bitstream Integration}
Using the proposed HLS methodology, we can reuse the same hardware infrastructure for a group of applications because only the context of the instruction memory in SCGRA needs to be updated. In order to avoid going through the entire ISE design flow, we obtain the organization of the instruction memory from XDL file \cite{beckhoff2011xilinx}, generate the corresponding BMM file, and integrate the instructions collected from the DFG scheduling into the pre-implemented SCGRA bitstream using the data2mem tool from Xilinx \cite{data2mem}. While original SCGRA design needs hours to implement, the bitstream updating scheme only costs a few seconds.


\section{SCGRA Design}\label{sec:scgra}
The SCGRA serves as the hardware infrastructure of the proposed HLS methodology. In order to meet the high performance computation requirements, it should be able to work at high frequency. Also it should be scalable in both hardware overhead and implementation frequency so that it can handle large applications using the proposed HLS methodology. With the two design goals in mind, a 2D Torus SCGRA is developed. It is composed of homogeneous PEs, so we can simply focus on the PE structure. The PE structure is shown in Figure \ref{fig:pe}. It basically consists of an ALU, a ROM based instruction memory and a multiple-port data memory. To connect with the system out of the FPGA, load/store data path is added to the PEs with IO interface. Details of the PE components are illustrated in the following sections.

\begin{figure*}[hbt]
\centering
\includegraphics[width=10cm]{pe}
\caption{PE structure}
\label{fig:pe}
\end{figure*}

\subsection{Instruction Memory and Data Memory}
The instruction memory adopts a ROM structure and it stores all the control words of the PE, so we simply use a counter to read instructions sequentially from it. When the instruction memory can be implemented using a single primitive BRAM, it is able to work at extreme frequency of the FPGA device. However, the timing gets pressing when the instruction memory becomes larger. In order to improve the timing of instruction memory, the Fixed Primitives scheme is adopted when we construct the instruction memory using core generator. Meanwhile, as a single instruction is distributed to multiple primitive BRAMs, it will be better to put control bits going to the same sub components of PE such as DSP48 to the same primitive BRAM accordingly. This strategy makes the placing and routing easier and hence improves the timing. In addition, registers can be added to the output port of the instruction memory to pipeline the critical paths originated from the instruction memory.


Data memory stores intermediate data which can either be forwarded to PE in downstream or sent to ALU for calculation. In order to parallelize both the forwarding and ALU calculation, we need at least four read ports while three of them are allocated to ALU and the rest one is used for forwarding.  Similarly, two write ports are needed to store input data from upstream memory and result of ALU at the same time. Since BRAM in Xilinx FPGA initially has read ports and write ports shared, a duplicated true dual port memory as shown in Figure \ref{fig:w2r4} is able to satisfy the requirement. However, there will be no write port available when ALU reads data for calculation, which can't meet the needs of fully pipelined ALU. In this case, maximum ALU throughput is cut down to be 0.5. To avoid this bottleneck, we duplicate another true dual port BRAM in the data memory as shown in Figure \ref{fig:w3r6}. Three read ports are allocated to ALU and the rest are used for forwarding.

\begin{figure}[hbt]
\centering
\subfloat[Two write ports and four read ports] {\label{fig:w2r4} 
\includegraphics[width=4cm]{w2r4}}\\
\subfloat[Three write ports and six read ports] {\label{fig:w3r6} 
\includegraphics[width=4cm]{w3r6}}
\caption{Multiple-port data memory}
\label{fig:datamemory}
\end{figure}

%\begin{figure}[hbt]
%\centering
%\begin{subfigure}{}
%\centering
%\includegraphics[width=4cm]{w2r4}
%\caption{Two write ports and four read ports}
%\label{fig:w2r4}
%\end{subfigure}
%\begin{subfigure}{}
%\centering
%\includegraphics[width=4cm]{w3r6}
%\caption{Three write ports and six read ports}
%\label{fig:w3r6}
%\end{subfigure}
%\caption{Multiple port data memory}
%\label{fig:datamemory}
%\end{figure}

\subsection{ALU}
Since the proposed HLS methodology targets at computation intensive application kernels, the operations needed are basically arithmetic operations and logic operations. Figure \ref{fig:ALU} shows a three-stage pipeline ALU template which is able to cover these operations. DSP core in Xilinx FPGA, which has already included quite a few operations such as multiply-add, is widely used in computation applications and it is able to operate at extreme speed configured with three-stage pipeline, so we integrate it in the ALU. For the operations that are not covered, a three-stage template $in1$ <$OP1$> $in2$ <$OP2$> $in3$ is developed. $OP1$ and $OP2$ can be any operations that fit in single-stage pipeline. When $OP1$ and $OP2$ are replaced with the operations such as add, sub, and xor, the template is also able to work at full speed. In addition, we also have a special PHI operation embedded in the template to handle the applications that have branches merged. PHI operation is actually a multiplexer, so it has no influence on the timing. 

\begin{figure}[hbt]
\centering
\includegraphics[width=8cm]{alu}
\caption{ALU Template}
\label{fig:ALU}
\end{figure} 

\subsection{Load/Store Interface}
When PE also serves as IO interface of SCGRA, load path or store path should be added. In this work, we assume that input data is organized as the scheduler required, so a single bit control signal is needed to decide whether there is a pop for load FIFO. Similarly, another single bit signal is used to decide whether there is a push for store FIFO. Since there are still reserved bits left in instruction memory, the control bits simply stored in instruction memory. Note that load path results in larger multiplexer, and additional pipeline register is added to solve this problem.


\section{EXPERIMENTS}\label{sec:results}
In this section, we take 5 computation kernels including matrix multiply (MM), fast Fourier transform (FFT), discrete convolution (CONV), advanced encryption standard (AES) and Viterbi decoder (VD) as our benchmark and implement them using both AUTOESL and the proposed HLS methodology. After that, we make a comprehensive comparison in terms of design efficiency, hardware implementation and performance.


\subsection{Benchmark}
MM has two input matrices $A \left( i,j \right)$, $B \left( j,k \right)$ and an output matrix $C \left( i,k \right)$
where $i=1,2,3,...,M$, $j=1,2,3,...,N$, $k=1,2,3,...,P$. It can be described as follow:\begin{displaymath} C \left( i,k \right) = \sum_{i=1}^N {A
\left( i,j \right) \times B(j,k)} \end{displaymath} In the experiment, we assume that $M=20, N=20, P=20$. And there are $M \times N \times P = 8000$ operations which are mainly multiply-add operations.


FFT initially takes vector $x \left( i \right)$ as input, and vector $X \left( i \right)$ as output where $i=1,2,3,...,N,N=2^M, M \in Z$. In order to escape $\sin$ and $\cos$ computation on FPGA, we also assume roots of unity vector to be input data which are simply determined by $N$. In this case, there are totally $1.5 \times N$ input data including $N$ input signal and $N/2$ roots of unity. We adopt the radix-2 Cooley-Tukey algorithm for implementation and set $N=1024,M=10$. And there are $N \times M=10240$ operations each of which is a complex multiply-accumulation with 3 input. Since we don't have the complex multiply-add DSP core available for AUTOESL implementation at the moment, we simply replace the complex multiply-add with real number multiply-add to keep the computation structure in the experiment.


Suppose input signal vectors of CONV are $u \left( i \right)$ and $h \left( i \right)$ where $i=1,2,3,...,N$. When $i>N$ or $i<1$, $u \left( i \right) = 0,h \left( i \right) = 0$.Then the output signal vector can be expressed as follow: \begin{displaymath} y \left( k \right) = \sum_{i=1}^N {u \left( i \right) \times h \left( k-i \right),k=N/2,N/2+1,...,3N/2} \end{displaymath} In the experiment, $N=100$ and there are around $3N^2/4$ operations which are mainly multiply-add operations.


AES is based on the principle named substitution-permutation network and it operates on a $4 \times 4$ column-major order matrix of bytes which is also called state. The encryption process basically consists of four steps. First of all, round keys are derived using Rijndael's key schedule. Secondly, each byte of the state is combined with the round key using bitwise xor. Thirdly, the plain text goes through byte substitution using Rijnhdael's S-box, row shifting, column mixing and round key adding in serial order, and then repeat all the processes except column mixing. In the experiment, input data consists of 10 128-bit blocks and the key adopts 128-bit option. In this case, there are 12660 operations mainly including circular shift and bitwise exclusive or.


VD can be used to decode the bitstream encoded by convolutional code. The basic Viterbi decoder operation is butterfly operation in trellis diagram. Each butterfly involves an Add-Compare-Select operation on the two nodes where the 0 and I paths from the current node merge at the next step of the trellis.  In the experiments, we adopt the NASA standard in which the decoding rate is $1/2$ and the decoding length is 7. Input message length is 150 bit and the total operation number is around 2989 including shift, add, or and compare.


\subsection{Experiment Setup}
All runtimes were obtained on a Linux workstation with an Intel(R) Xeon(R) CPU E5345 and 8GB of RAM. LLVM v3.2 and clang v3.2 were used for C program compiling. AUTOESL 2011.4.2 and PlanAheadv13.4 were used for synthesis and implementation respectively. All the designs targeted at Virtex6 (xc6vlx240tff784-1).In order to have a comprehensive comparison, we provide three implementations for each computation kernel with different tradeoff between performance and hardware overhead using both AUTOESL and the proposed HLS methodology. 


AUTOESL makes the trade-off between overhead and performance mainly by changing the loop unrolling factor, so implementation without unrolling, implementation with medium unrolling and implementation with large unrolling are presented. Configurations of each kind of unrolling for each computation kernel are detailed in Table \ref{tab:urconfig}. Also all the implementations are set to be pipelined using AUTOESL directive. In addition, we put all the input arrays/vectors in a single vector and output arrays/vectors in another vector to make sure AUTOESL generate a single input port and a single output port for a fair comparison. 

\begin{table}[htb]
\caption{Detailed unrolling configuration of the benchmark}
\label{tab:urconfig}
\centering
\begin{tabular}{|p{1cm}|p{6.5cm}|}
\hline
MM & {It is a three-level nested loop and each loop iterates 20. Unrolling factors of three implementations are  $1 \times 1 \times 1$, $1 \times 5 \times 20$ and $1 \times 10 \times 20$. When I set unrolling factor to be $1 \times 20 \times 20$, AUTOESL fails to provide better performance.}\\

\hline
FFT & {It is a three-level nested loop. The outside loop iteration number is 10 while the iteration number of the rest two loops varies with the outside loop. Unrolling factors of the three implementations are $1 \times 1 \times 1$, $10 \times 1 \times 1$ and $10 \times m \times n$. Although both $m$ and $n$ change all the time, $m \times n$ is fixed to be 32. AUTOESL fails to complete the implementation when I further increase the unrolling factors.}\\

\hline
CONV & {It is a two-level nested loop and each loop iterates 100. Since part of the element is 0, simple branch helps to reduce the multiplication operation. Unrolling factors of the three implementations are $1 \times 1$, $1 \times 100$, $5 \times 100$. And target FPGA device has insufficient DSP to accommodate further unrolling.}\\

\hline
AES & {AES essentially consists of key expansion initially, and then repeated kernel procedure including byte substitution, row shifting, column mixing, and round key addition. The kernel procedure iterates 9. Unrolling factors of the three implementations are 1, 3, and 9. I tried to unroll and inline the sub functions manually, AUTOESL failed to implement the design.}\\

\hline
VD & {VD kernel is a loop of a series of butterfly operations in trellis diagram. The loop iterates 150. Unrolling factors of the three implementations are 1, 2 and 5.}\\

\hline
\end{tabular}
\end{table}

The proposed HLS methodology compromises through changing the SCGRA scale. In the experiments, $4 \times 4$ Torus, $3 \times 3$ Torus and $2 \times 2$ Torus are adopted and each of the SCGRA also has implementations with 1K instruction memory, 2K instruction memory and 4K instruction memory respectively. Data memory of each PE is set $256 \times 16$-bit and each data memory port requires a 8-bit address. ALU supports 8 types of operations and the opcode takes 3-bit. With the selection signals of the multiplexers included, the instruction width of the PE without IO port should be at least 65-bit. and the PE with IO needs one more bit for load/store FIFO operation. As the primitive BRAM in Xilinx FPGA is 18K-bit/36K-bit, we actually build 72-bit width instruction memory. 


\subsection{Experiment Results}
\subsubsection{Design Efficiency}
The process transforming HLL program all the way to bitstream will repeat many times using the HLS tools especially at the debugging period, and it takes majority of the time brought by the HLS tools through the design flow. Therefore, we take the design time that the HLS tools consume in this period as the metric of design tools' efficiency.


Figure \ref{fig:designtimeCP} shows the design time of the benchmark using both AUTOESL and the proposed HLS methodology. Even though no unrolling directive is added, it still takes AUTOESL around 7 minutes to implement a simple design. When the directive with large unrolling factors is employed, it usually costs AUTOESL more than 20 minutes to implement the design. The situation will be even worse when the timing constrain is pressing and hardware overhead is relatively large. Using the proposed HLS methodology, even FFT and VD which costs AUTOESL 2 hours can be implemented within a minute. According to the experiments, the proposed HLS methodology is essentially 10X ~ 100X faster than AUTOESL.AUTOESL basically includes AUTOESL synthesis and AUTOESL implementation, while the proposed HLS methodology consists of LLVM compiling (DFG generation), SCGRA scheduling (instruction generation), and bitstream integration (merging the instructions and the pre-implemented SCGRA bitstream).

\begin{figure*}[hbt]
\centering
\includegraphics[width=12cm]{designtimeCP}
\caption{Design time comparison of the benchmark using both AUTOESL and the proposed HLS methodology}
\label{fig:designtimeCP}
\end{figure*} 

\subsubsection{Hardware Implementation}
In this section, hardware implementation including both hardware resource overhead and implementation frequency are compared.
Table \ref{tab:overhead} presents the hardware overhead of all the benchmark using the proposed HLS methodology with a $4 \times4$ Torus(P44), the proposed HLS methodology with a $3 \times 3$ Torus (P33), the proposed HLS methodology with $2 \times 2$ Torus (P22), AUTOESL without unrolling (no unrolling, NUR), AUTOESL with medium unrolling (MUR), and AUTOESL with large unrolling (LUR) respectively. It can be seen that AUTOESL has relatively compact implementation when the unrolling factor is small. Also AUTOESL tends to use less BRAM blocks compared to the proposed HLS method. The reason probably lies in that most of the kernels in the benchmark have inner loop fully unrolled and there is little block to block communication. However, hardware overhead of AUTOESL in terms of SLICE, LUT, FF and DSP (whenever it is needed) increases dramatically when source code of the benchmark is expanded. And soon the proposed HLS method consumes less SLICE, LUT, FF and DSP than AUTOESL method does for implementing the MM, FFT, and CONV. Compared with AUTOESL, the proposed HLS method merely need half of the SLICE, LUT, FF and 1/20 ~ 1/50 of DSP. The situation is different in AES implementation because the SCGRA has more computation capability than that is required. Pre-implementing more light-weight SCGRA may decrease the overhead.

\begin{table}[htb]
\caption{Hardware overhead of the benchmark.}
\label{tab:overhead}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{} & SLICE & LUT & FF & DSP & RAM\\
\hline
\multirow{4}{*}{MM} & T44 & 2273 & 4303 & 11129 & 16 & 176\\
\cline{2-7}
& T33 & 1029 & 3229 & 6958 & 9 & 99\\
\cline{2-7}
& T22 & 619 & 1045 & 2805 & 4 & 76\\
\cline{2-7}
& NUR & 158 & 296 & 320 & 1 & 2\\
\cline{2-7}
& MUR & 1151 & 3517 & 4224 & 78 & 2\\
\cline{2-7}
& LUR & 1958	& 6711	& 8006	&182 &2\\
\hline

\multirow{4}{*}{FFT}	& T44 & 2870 & 3905 & 12380 & 16 & 304\\
\cline{2-7}
& T33 & 1519 & 2574 & 6976 & 9 & 171\\
\cline{2-7}
& T22 & - & - & - & - & -\\
\cline{2-7}

& NUR & 315 & 655 & 889 & 2 & 2\\
\cline{2-7}
& MUR & 648 & 1889 & 1770 &	10 & 11\\
\cline{2-7}
& LUR &	10097 &	30809 &	20153 &	173 & 12\\
\hline

\multirow{4}{*}{CONV} & T44 & 2273 & 4303 & 11129 &	16 & 176\\
\cline{2-7}
& T33 & 1029 & 3229 & 6958 & 9 & 99\\
\cline{2-7}
& T22 & 619 & 1045 & 2805 & 4 & 76\\
\cline{2-7}

& NUR &	87 & 217 & 205 & 1 & 2\\
\cline{2-7}
& MUR & 1149 & 3763 &	4558 & 98 &	2\\
\cline{2-7}
& LUR & 4213 & 13633 & 19940 & 497 & 2\\
\hline

\multirow{4}{*}{AES} & T44 & 2273 & 4303 & 11129 & 16 & 176\\
\cline{2-7}
& T33 & 1029 & 3229 & 6958 & 9 & 99\\
\cline{2-7}
& T22 & - & - & - & - & -\\
\cline{2-7}

& NUR &	501 & 1251 & 1660 & 0 & 8\\
\cline{2-7}
& MUR &	705 & 1799 & 1947 &	0 &	8\\
\cline{2-7}
& LUR & 708 & 1767 & 1872 & 0 & 8\\
\hline

\multirow{4}{*}{VD} & T44 & - & - & - & - & -\\
\cline{2-7}
& T33 & - & - & - & - & -\\
\cline{2-7}
& T22 & - & - & - & - & -\\
\cline{2-7}

& NUR & 1911 & 4883 & 6684 & 0 & 12\\
\cline{2-7}
& MUR & 3306 & 10033 & 12863 & 0 & 12\\
\cline{2-7}
& LUR & 8833 & 24693 & 31886 & 0 & 12\\
\hline
\end{tabular}
\end{table}

Figure \ref{fig:frequencyCP} shows the implementation frequency using both HLS tools. It is clear that the SCGRA employed in the proposed HLS method can be implemented at more than 400MHz which gets near the extreme frequency of three-stage-pipeline DSP core implemented in the same device. Note that MM, CONV and AES use the same SCGRA with 2K$\times$72-bit instruction rom for each PE while FFT needs the SCGRA with 4K$\times$72-bit instruction rom. VD requires even larger instruction rom which we have not prepared in advance, so it is left blank and we consider it as an implementation failure. AUTOESL has specific implementation for each benchmark, but the highest implementation frequency is only around 300MHz. At the same time, the implementation frequency deteriorates radically with the increase of loop unrolling factors. The worst implementation is even less than 100MHz.
\begin{figure*}[hbt]
\centering
\includegraphics[width=12cm]{freqCP}
\caption{Implementation frequency comparison of the benchmark using both AUTOESL and the proposed HLS methodology}
\label{fig:frequencyCP}
\end{figure*}

\subsubsection{Performance}
In this section, the execution time of the benchmark is taken as the metric of performance and it is actually the production of execution cycles and implemented clock period. AUTOESL acquires the execution cycles through AUTOESL synthesis, and the proposed HLS method obtains the execution cycles from the SCGRA scheduler. All clock periods of the implementations come from FPGA implementation. When the proposed HLS method fails to find a SCGRA from all the pre-implemented SCGRA versions, we consider it as implementation failure and the performance is left blank. In this experiment, VD under all kinds of SCGRA and FFT under $2\times2$ Torus SCGRA come across the occasion. Also note that AUTOESL fails to analyze the FFT performance when there is no unrolling at all, so the performance is also left blank.


Figure \ref{fig:performanceCP} shows the performance comparison of the benchmark using different HLS tools. It can be seen that the proposed HLS method outperforms AUTOESL in all the benchmark except the one it fails to implement. The highest performance of MM, FFT, CONV and AES using the proposed HLS method is around 7X, 19X, 5X and 30X faster than that using AUTOESL respectively. One of the most critical reasons lies in that AUTOESL adopts RAM buffer as communication interfaces between blocks which is incapable to provide enough bandwidth to support large parallelism exploration. Take the experiments of FFT and AES as an example. Both computation kernels of FFT and AES are divided into multiple sub blocks to provide larger parallelism for AUTOESL implementation. However, the dependent sub blocks have to fetch data from RAM buffer in upstream and send data to RAM buffer in downstream. As a result, each sub block has additional IO bandwidth constrains which negatively influence the parallelism exploration, and therefore the design with larger unrolling factors fails to improve the performance much. At the same time, the design with larger unrolling requires larger hardware resource and deteriorates the timing. The two factors induce that performance of FFT and AES even degrade when larger unrolling factors are employed. The proposed HLS method merely has IO constrains and all the inner operations of the applications share the whole communication network. Consequently, it provides better performance.

\begin{figure*}[hbt]
\centering
\includegraphics[width=12cm]{performanceCP}
\caption{Performance comparison of the benchmark using both AUTOESL and the proposed HLS methodology}
\label{fig:performanceCP}
\end{figure*}

\section{Conclusions}\label{sec:conclusions}
In this paper, we have proposed a SCGRA based HLS method particularly for FPGA design. Using this method, HLL program is transformed to a DFG first, and then the DFG is scheduled to a SCGRA. After that, the scheduling result is integrated into the pre-implemented SCGRA bitstream and the updated new bitstream can be downloaded to FPGA directly. Since the proposed HLS method bypasses the traditionally RTL compiling step, the design time is significantly decreased. Meanwhile, SCGRA based computation scheme extensively develops the parallelism of the computation intensive kernels and the well-defined SCGRA structure facilitates a high frequency implementation. Therefore, the computation kernels implemented using the HLS method is able to achieve pretty high performance. Experiment results show that the new HLS method outperforms AUTOESL by 10X ~ 100X in terms of design efficiency and 5X ~ 30X in terms of performance. As for hardware overhead, the implementations using the proposed HLS method consume more BRAM but less SLICE, LUT, FF and DSP compared with the implementations using AUTOESL with relatively large unrolling.

